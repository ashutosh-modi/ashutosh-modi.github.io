---
layout: default
title: Ashutosh Modi
---
 <!-- source: https://www.w3schools.com/howto/howto_js_tabs.asp -->
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
body {font-family: Arial;}

/* Style the tab */
.tab {
  overflow: hidden;
  border: 1px solid #ccc;
  background-color: #f1f1f1;
}

/* Style the buttons inside the tab */
.tab button {
  background-color: inherit;
  float: left;
  border: none;
  outline: none;
  cursor: pointer;
  padding: 14px 16px;
  transition: 0.3s;
  font-size: 17px;
}

/* Change background color of buttons on hover */
.tab button:hover {
  background-color: orange;/*#ddd;*/
}

/* Create an active/current tablink class */
.tab button.active {
  background-color: orange;/*#ccc;*/
}

/* Style the tab content */
.tabcontent {
  display: none;
  padding: 6px 12px;
  border: 1px solid #ccc;
  border-top: none;
}

/* ######################### */ 
/* Tables */
/* https://www.w3schools.com/html/html_tables.asp */ 
table {
  font-family: arial, sans-serif;
  border-collapse: collapse;
  width: 100%;
}

td {
  border: 1px solid #dddddd;
  text-align: left;
  padding: 8px;
}

th {
  border: 1px solid #dddddd;
  text-align: center;
  padding: 8px;
  background-color: #4CAF50;
  color: black;
}

tr:nth-child(even) {
  background-color: #dddddd;
} 
 
/* ######################### */ 
/* Collapsible */
/* https://www.w3schools.com/howto/howto_js_collapsible.asp */

.collapsible {
  background-color: white; /* #4CAF50; #777;*/
  color: black;
  cursor: pointer;
  padding: 5px;
  width: 70px;/*15%;*/
  border: none;
  text-align: left;
  outline: none;
  font-size: 13px;
}

.active, .collapsible:hover {
  background-color: orange;/*#555;*/
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
  background-color: #f1f1f1;
  font-size: 13px;
}

</style>
</head>
<body>

<!--  ######################### -->

<!-- 
<h2>Tabs</h2>
-->
<h2 style="color: orange;"> Special Topics in Natural Language Processing (CS698O) : Winter 2020 </h2>

Natural language (NL) refers to the language spoken/written by humans. NL is the primary mode of communication for humans. With the growth of the world wide web, data in the form of textual natural language has grown exponentially. It calls for the development of algorithms and techniques for processing natural language for automation and the development of intelligent machines. This course will primarily focus on understanding and developing techniques/learning algorithms/models for processing text. We will have a statistical approach to Natural Language Processing (NLP), wherein we will learn how one could develop natural language understanding models from regularities in large corpora of natural language texts.

</br> </br>

<div class="tab">
  <button class="tablinks" onclick="pub(event, 'logistics')"> <font style="color: blue;"> Information and Logistics </font></button>
  <button class="tablinks" onclick="pub(event, 'lectures')"> <font style="color: blue;"> Syllabus and Lectures </font></button>	
  <button class="tablinks" onclick="pub(event, 'projects')"> <font style="color: blue;"> Projects </font></button>	
</div>

<!-- ############################################################################################################### -->

<!-- Logistics -->
<div id="logistics" class="tabcontent">
  <!-- <h3 style="color: orange;"> Information and Logistics</h3> -->

<h4 style="color: orange;"> Pre-requisites:</h4>

        <b> Must: </b> Introduction to Machine Learning (CS771) or equivalent course, Proficiency in Linear Algebra, Probability and Statistics, Proficiency in Python Programming <br>

        <b> Desirable: </b> Probabilistic Machine Learning (CS772), Topics in Probabilistic Modeling and Inference (CS775), Deep Learning for Computer Vision (CS776)


<h4 style="color: orange;"> Course Instructor: </h4>  <a href = "https://ashutosh-modi.github.io/"> Dr. Ashutosh Modi</a> 

<h4 style="color: orange;"> Course TAs: </h4>
 
            Samik Some (Email:  <a href = "mailto:samik@cse.iitk.ac.in?subject = Feedback&body = Message"> samik@cse.iitk.ac.in </a> ) <br>
            Avideep Mukherjee (Email:  <a href = "mailto:avideep@cse.iitk.ac.in?subject = Feedback&body = Message"> avideep@cse.iitk.ac.in </a> ) <br>
            Munender Kumar Varshney (Email:  <a href = "mailto:munenderv@cse.iitk.ac.in?subject = Feedback&body = Message"> munenderv@cse.iitk.ac.in </a> ) <br>
            Naman Narang (Email:  <a href = "mailto:namann@cse.iitk.ac.in?subject = Feedback&body = Message"> namann@cse.iitk.ac.in </a> ) <br>
            Nitin Vivek Bharti (Email:  <a href = "mailto:nvbnitin@iitk.ac.in?subject = Feedback&body = Message"> nvbnitin@iitk.ac.in </a> ) <br>
            
<h4 style="color: orange;"> Course Email: </h4>

        In  case  you  want  to  communicate  with  the  instructor,  please  do  not  send  any  direct emails to the instructor (these will most likely end in spam), use this course email for the communication: <a href = "mailto:nlp.course.iitk@gmail.com ?subject = Feedback&body = Message"> nlp.course.iitk@gmail.com </a>

<h4 style="color: orange;"> Weekly Sessions: </h4> 
Monday 10-11AM <br>
Wednesday 10-11AM <br>
Friday 12-1PM <br>

<h4 style="color: orange;"> Lecture Venue: </h4>
<a href = "https://goo.gl/maps/AVgx49xBfHyA3BGN8"> Lecture Hall 11 </a>

<h4 style="color: orange;"> Course Annoucements: </h4>
The course will be a managed via <a href="piazza.com/indian_institute_of_technology_kanpur/winter2020/cs698o/home">Piazza</a>. Please sign-up on <a href="piazza.com/indian_institute_of_technology_kanpur/winter2020/cs698o">Piazza</a> for course annoucements, study material, and resources. For the access code, please contact the instructor or one of the TAs. 

<h4 style="color: orange;"> Grading: </h4>

        This is a research project oriented course and the project carries the maximum weightage.  Given that course is going to be online, all exams will be conducted online. The tentative weightage for different components are as follows.  Please note that this grading scheme is tentative (due to COVID uncertainties and factors beyond Instructor’s control), and weightage might change. <br><br>
        Quizzes:  40% <br>
        Project:  60% <br>
        Mid-Sem Exam:  Project paper and presentation <br>
        End-Sem Exam:  Project paper and presentation <br>

</br></br>


</div>

<!-- ############################################################################################################### -->
<!-- Syllabus -->

<div id="lectures" class="tabcontent">
<!--   <h3 style="color: orange;">Syllabus and Lectures</h3> -->
  
 <h4 style="color: orange;"> Lectures </h3>
 
 <table>
  <tr>
    <th>Date</th>
    <th>Topic</th>
    <!-- <th>References</th> -->
  </tr>
  <!-- Lecture 1 -->
  <tr>
    <td>03/01/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k4xylzgdicy66m"> Introduction and Logistics </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 2 -->
  <tr>
    <td>06/01/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k52226g6uvh3ll"> Why NLP is Hard and Linguistic Fundamentals-1 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 3 -->
  <tr>
    <td>08/01/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k5531p25llo4v3"> Linguistic Fundamentals-2 and NLP Pipeline </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 4 -->
  <tr>
    <td>10/01/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k5anfatvhtj50x"> Language Models 1 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 5 -->
  <tr>
    <td>13/01/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k5r221ujd156qq"> Language Models 2 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 6 -->
  <tr>
    <td>15/01/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k5ey3wi94e928j"> Language Models 3 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 7 -->
  <tr>
    <td>17/01/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k5m1jxd9tiu3h1"> Language Models 4 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 8 -->
  <tr>
    <td>20/01/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k5mt27b5dwvq8"> Language Models Pre-Finale </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 9 -->
  <tr>
    <td>22/01/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k5p3xnrln5g52o"> Language Models Finale </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 10 -->
  <tr>
    <td>27/01/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k5wntczdh7b4hh"> Sequence Prediction 1 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 11 -->
  <tr>
    <td>29/01/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k68x1f5ebg8jv"> Sequence Prediction 2 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 12 -->
  <tr>
    <td>31/01/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k68x0w3nd91790"> Sequence Prediction 3 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 13 -->
  <tr>
    <td>03/02/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k66px8n9ws66uf"> Sequence Prediction 4 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 14 -->
  <tr>
    <td>05/02/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k68y31sqle94q6"> Sequence Prediction 5 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 15 -->
  <tr>
    <td>07/02/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k6eod6q5tou541"> Parsing 1 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 16 -->
  <tr>
    <td>10/02/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k6g5s6n0ck5172"> Parsing 2 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 17 -->
  <tr>
    <td>12/02/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k6j38x09xxn3cg"> Parsing 3 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 18 -->
  <tr>
    <td>14/02/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k6z6qppcjaz215"> Parsing 4 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Mid-Sem -->
  <tr>
    <td> 16/02/2020 - 22/02/2020 </td>
    <td> Mid-Semester Exams: Project Presentations </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 19 -->
  <tr>
    <td>24/02/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k702lf07lx37h1"> Parsing 5 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 20 -->
  <tr>
    <td>26/02/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k732238e2y441t"> Parsing 6 </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 21 -->
  <tr>
    <td>04/03/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k7d5s177w9d5to"> Naïve Bayes and EM Algorithm </a></td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 22 -->
  <tr>
    <td>06/03/2020</td>
    <td> <a href="https://piazza.com/class_profile/get_resource/k4v0nxex5zz62c/k7d5s177w9d5to"> Distributional Semantics </a></td>
    <!-- <td>-</td> -->
  </tr>
 <!-- Spring Break -->
  <tr>
    <td>09/03/2020 - 13/03/2020</td>
    <td> Spring Break </td>
    <!-- <td>-</td> -->
  </tr>
  <!-- Lecture 21 -->
  <tr>
    <td> 16/03/2020 -  </td>
    <td> Semester Terminated due to <a href="https://en.wikipedia.org/wiki/Coronavirus_disease_2019">COVID-19</a> Lockdown </td>
    <!-- <td>-</td> -->
  </tr>
  
</table>
<br> <br>

<!-- 
  <h4 style="color: orange;"> Course Contents: </h4>

   Tentative list of topics we will be covering in this course: 
   
        <ol>
          <li> Introduction to Natural Language (NL): why is it hard to process NL, linguistics fundamentals, etc.</li>
          <li> Language Models:  n-grams, smoothing, class-based, brown clustering</li>
          <li> Sequence Labeling:  HMM, MaxEnt, CRFs, related applications of these models e.g.  Part of Speech tagging, etc.</li>
          <li> Parsing:  CFG, Lexicalized CFG, PCFGs, Dependency parsing</li>
          <li>Applications:   Named  Entity  Recognition,  Coreference  Resolution,  text  classification,  toolkits  e.g., Spacy, etc.</li>
          <li>Distributional Semantics:  distributional hypothesis, vector space models, etc.</li>
          <li>Distributed Representations: Neural Networks (NN), Backpropogation, Softmax, Hierarchical Softmax</li>
          <li> Word Vectors:  Feedforward NN, Word2Vec, GloVE, Contextualization (ELMo etc.), Subword information (FastText, etc.)</li>
          <li>Deep Models:  RNNs, LSTMs, Attention, CNNs, applications in language, etc.</li>
          <li>Sequence to Sequence models:  machine translation and other applications</li>
          <li>Transformers:  BERT, transfer learning and applications</li>
        </ol>

-->
        <h4 style="color: orange;"> References: </h4> 
 There are no specific references, this course gleans information from a variety of sources likebooks, research papers, other courses, etc.  Relevant references would be suggested in the lectures.  Some of the frequent references are as follows:

       <ol>
          <li> Speech and Language Processing, Daniel Jurafsky, James H.Martin</li>
          <li>  Foundations of Statistical Natural Language Processing, CH Manning, H Schtze</li>
          <li> Introduction to Natural Language Processing, Jacob Eisenstein</li>
          <li>  Natural Language Understanding, James Allen</li>
        </ol> 


<h4 style="color: orange;"> Useful Resources: </h4> 
     <ol>
     <li> <a href="https://www.aclweb.org/anthology/">ACL Anthology: Repo for NLP Research Papers</a> </li>
     <li> <a href="https://github.com/allenai/writing-code-for-nlp-research-emnlp2018/blob/master/writing_code_for_nlp_research.pdf">Writing Code for NLP Research</a></li>
     <li> <a href="https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf">Deep Learning with PyTorch Book</a> </li>
     <li> <a href="https://spacy.io/">Spacy NLP Toolkit</a> </li>
     <li> <a href="http://joschu.net/blog/opinionated-guide-ml-research.html">Guide To ML Research</a> </li>
     </ol> 

</div>  

<!-- ############################################################################################################### -->
<!-- Projects -->

<div id="projects" class="tabcontent">
  <!-- <h3 style="color: orange;">Projects</h3> -->
  
<!--  <h4 style="color: green;">Projects list coming up soon</h4> <br> -->
 
<!-- *************************************** -->
 
<!-- <h4 style="color: blue;"> Commonsense Validation and Explanation </h4> 
 <font color="blue"> <b> Commonsense Validation and Explanation </b></font> <br>
 Sandeep Routray, Soumya Ranjan Dash, Prateek Varshney <br>
 <a href="https://arxiv.org/abs/2007.10830"> PAPER </a>  &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
 -->
 <font color="blue"> <b> Commonsense Validation and Explanation </b></font><br>
Sandeep Routray, Soumya Ranjan Dash, Prateek Varshney <br>
<a href="https://arxiv.org/abs/2007.10830"><tt><b>PAPER</b></tt></a> &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 
 <button type="button" class="collapsible"><tt><b>ABSTRACT</b></tt></button>
<div class="content">
  <p>In this project, we develop a system for addressing the research problem posed in Task 4 of SemEval 2020, which involves differentiating between natural language statements that confirm to common sense and those that do not. The organizers propose three subtasks - first, selecting between two sentences, the one which is against common sense. Second, identifying the most crucial reason why a statement does not make sense. Third, generating novel reasons for explaining the against common sense statement. Out of the three subtasks, this paper reports the system description of subtask A and subtask B. This paper proposes a model based on transformer neural network architecture for addressing the subtasks. The novelty in work lies in the architecture design, which handles the logical implication of contradicting statements and simultaneous information extraction from both sentences. We use a parallel instance of transformers, which is responsible for a boost in the performance. We achieved an accuracy of 94.8% in subtask A and 89% in subtask B on the test set. </p>
</div>
<br>
<br> 
<!-- *************************************** -->
 
  <font color="blue"> <b> Sentiment Analysis of Code Mixed Text </b></font><br>
Ayush Kumar, Harsh Agarwal, Keshav Bansal<br>
<a href="https://arxiv.org/abs/2007.10819"><tt><b>PAPER</b></tt></a> &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 
 <button type="button" class="collapsible"><tt><b>ABSTRACT</b></tt></button>
<div class="content">
  <p>Sentiment Analysis of code-mixed text has diversified applications in opinion mining ranging from tagging user reviews to identifying social or political sentiments of a sub-population. In this paper, we present an ensemble architecture of convolutional neural net (CNN) and self-attention based LSTM for sentiment analysis of code-mixed tweets. While the CNN component helps in the classification of positive and negative tweets, the self-attention based LSTM, helps in the classification of neutral tweets, because of its ability to identify correct sentiment among multiple sentiment bearing units. We achieved F1 scores of 0.707 (ranked 5th) and 0.725 (ranked 13th) on Hindi-English (Hinglish) and Spanish-English (Spanglish) datasets, respectively. The submissions for Hinglish and Spanglish tasks were made under the usernames ayushk and harsh_6 respectively.  </p>
</div>
 <br>
<br> 
<!-- *************************************** -->

   <font color="blue"> <b> Modelling Causal Reasoning in Language, Detecting Counterfactuals Application: Financial Document Causality Detection </b></font><br>
Rohin Garg, Shashank Gupta, Anirudh Anil Ojha<br>
<a href="https://arxiv.org/abs/2007.10866"><tt><b>PAPER</b></tt></a> &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 
 <button type="button" class="collapsible"><tt><b>ABSTRACT</b></tt></button>
<div class="content">
  <p>This project aims at developing computational models for detecting a class of textual expressions known as counterfactuals and separating them into their constituent elements. Counterfactual statements describe events that have not or could not have occurred and the possible implications of such events. While counterfactual reasoning is natural for humans, understanding these expressions is difficult for artificial agents due to a variety of linguistic subtleties. For this project, we participated in Task 5 of SemEval-2020. Our final submitted approaches were an ensemble of various fine-tuned transformer-based and CNN-based models for the first subtask and a transformer model with dependency tree information for the second subtask. We ranked 4-th and 9-th in the overall leaderboard. We also explored various other approaches that involved the use of classical methods, other neural architectures and the incorporation of different linguistic features.  </p>
</div>
 <br>
<br> 
<!-- *************************************** -->
 
   <font color="blue"> <b> Detection of Propaganda Techniques in News Articles </b></font><br>
Paramansh Singh, Siraj Singh Sandhu, Subham Kumar<br>
<a href="https://arxiv.org/abs/2007.10827"><tt><b>PAPER</b></tt></a> &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 
 <button type="button" class="collapsible"><tt><b>ABSTRACT</b></tt></button>
<div class="content">
  <p>In this project, we develop a system for addressing the research problem posed in SemEval 2020 Task 11: Detection of Propaganda Techniques in News Articles for each of the two subtasks of Span Identification and Technique Classification. We make use of pre-trained BERT language model enhanced with tagging techniques developed for the task of Named Entity Recognition (NER), to develop a system for identifying propaganda spans in the text. For the second subtask, we incorporate contextual features in a pre-trained RoBERTa model for the classification of propaganda techniques. We were ranked 5th in the propaganda technique classification subtask.   </p>
</div>
 <br>
<br> 
<!-- *************************************** -->
  
   <font color="blue"> <b> Emphasis Selection for written text in visual media </b></font><br>
Rishabh Agarwal, Vipul Singhal, Sahil Dhull<br>
<a href="https://arxiv.org/abs/2007.10820"><tt><b>PAPER</b></tt></a> &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 
 <button type="button" class="collapsible"><tt><b>ABSTRACT</b></tt></button>
<div class="content">
  <p>In this project, we develop a system for addressing the research problem posed in Task 10 of SemEval-2020: Emphasis Selection For Written Text in Visual Media. We propose an end-to-end model that takes as input the text and corresponding to each word gives the probability of the word to be emphasized. Our results show that transformer-based models are particularly effective in this task. We achieved the best Matchm score (described in section 2.2) of 0.810 and were ranked third on the leaderboard.  </p>
</div>
 <br>
<br> 
<!-- *************************************** -->
  
   <font color="blue"> <b> Memotion Analysis </b></font><br>
Vishal Keswani, Sakshi Singh, Suryansh Agarwal<br>
<a href="https://arxiv.org/abs/2007.10822"><tt><b>PAPER</b></tt></a> &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 
 <button type="button" class="collapsible"><tt><b>ABSTRACT</b></tt></button>
<div class="content">
  <p>Social media is abundant in visual and textual information presented together or in isolation. Memes are the most popular form, belonging to the former class. In this project, we develop approaches for the Memotion Analysis problem as posed in SemEval-2020 Task 8. The goal of this task is to classify memes based on their emotional content and sentiment. We leverage techniques from Natural Language Processing (NLP) and Computer Vision (CV) towards the sentiment classification of internet memes (Subtask A). We consider Bimodal (text and image) as well as Unimodal (text-only) techniques in our study ranging from the Naïve Bayes classifier to Transformer-based approaches. Our results show that a text-only approach, a simple Feed Forward Neural Network (FFNN) with Word2vec embeddings as input, performs superior to all the others. We stand first in the Sentiment analysis task with a relative improvement of 63% over the baseline macro-F1 score. Our work is relevant to any task concerned with the combination of different modalities.   </p>
</div>
 <br>
<br> 
<!-- *************************************** --> 
  
   <font color="blue"> <b> Multilingual Offensive Language Identification in Social Media </b></font><br>
Karishma Laud, Jagriti Singh, Randeep Kumar Sahu<br>
<a href="https://arxiv.org/abs/2007.10877"><tt><b>PAPER</b></tt></a> &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 
 <button type="button" class="collapsible"><tt><b>ABSTRACT</b></tt></button>
<div class="content">
  <p>In this project, we develop for addressing the research problem posed in SemEval-2020 Shared Task 12 Multilingual Offensive Language Identification in Social Media. We participated in all the three sub-tasks of OffensEval-2020, and our final submissions during the evaluation phase included transformer-based approaches and a soft label-based approach. BERT based fine-tuned models were submitted for each language of sub-task A (offensive tweet identification). RoBERTa based fine-tuned model for sub-task B (automatic categorization of offense types) was submitted. We submitted two models for sub-task C (offense target identification), one using soft labels and the other using BERT based fine-tuned model. Our ranks for sub-task A were Greek-19 out of 37, Turkish-22 out of 46, Danish-26 out of 39, Arabic-39 out of 53, and English-20 out of 85. We achieved a rank of 28 out of 43 for sub-task B. Our best rank for sub-task C was 20 out of 39 using BERT based fine-tuned model.   </p>
</div>
 <br>
<br> 
<!-- *************************************** -->  
   
<font color="blue"> <b> Emotion-Cause Pair Extraction </b></font><br>
Aaditya Singh, Shreeshail Hingane, Saim Wani<br>
<button type="button" class="collapsible"><tt><b>ABSTRACT</b></tt></button>
<div class="content">
 <p>The task of emotion cause extraction (ECE) is aimed at inferring the cause of an emotion expressed through a piece of text. The task assumes that the emotion associated with the text is provided to us. Such an emotion→cause extraction pipeline disregards the inherent dependence between emotions and causes while also limiting the applicability of the model. Recent work in emotion-cause pair extraction (ECPE) (Xia and Ding, 2019) has tried to improve upon this by extracting emotion-cause clause pairs from the document in a 2-step approach—by first extracting emotion and cause clauses and then conducting emotion-cause pairing. However, this overlooks the effect that a cause clause has on the perception of the emotion since clause extraction happens in isolation from the pairing task. Further, it runs the risk of failing to extract potential emotion clauses in the first step of the pipeline—certain clauses do not appear to convey an emotion when seen in isolation from the cause clause. To overcome these drawbacks, we propose an end-to-end emotion-cause pair extraction architecture that infers emotion-cause pairs from documents and takes into account the effect of cause clause on the perceived emotion of the emotion clause. We evaluate our approach on the benchmark emotion cause dataset introduced in (Gui et al., 2016) and show significant performance improvements in the emotion-cause pairing task.    </p>
</div>
 <br>
<br> 
<!-- *************************************** -->  
   
<font color="blue"> <b> Affective Language Modelling and Text Generation </b></font><br>
Ahsan Barkati, Ishika Singh, Tushar Goswamy <br>
<button type="button" class="collapsible"><tt><b>ABSTRACT</b></tt></button>
<div class="content">
 <p>Messages for human conversation are best conveyed by flavouring the sentences with emotionally coloured words. In this project, we aim to integrate the affective sentence generation methodology to the state-of-the-art language generation models. It is intended to develop a model capable of generating affect-driven sentences without losing the grammatical correctness. We propose to incorporate emotion as prior for the probabilistic state-of-the-art sentence generation models such as GPT-2 and BERT. The model will give user the flexibility to control the category and intensity of emotion as well as the subject of the generated text. This is followed by demonstrating an application of the language model for story generation, advertisements and conversational agents for therapy chatbots.  </p>
</div>
 <br>
<br> 
<!-- *************************************** -->   
 
</div>  

<!-- ###########################################    SCRIPTS    ##################################################  -->
<!-- ##################################################  -->
<!-- Tab Java Script -->

<script>
function pub(evt, type) {
  var i, tabcontent, tablinks;
  tabcontent = document.getElementsByClassName("tabcontent");
  for (i = 0; i < tabcontent.length; i++) {
    tabcontent[i].style.display = "none";
  }
  tablinks = document.getElementsByClassName("tablinks");
  for (i = 0; i < tablinks.length; i++) {
    tablinks[i].className = tablinks[i].className.replace(" active", "");
  }
  document.getElementById(type).style.display = "block";
  evt.currentTarget.className += " active";
}
</script>

<!-- ##################################################  -->
<!-- Collapsible Java Script -->

<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>

<!-- ##################################################  -->

</body>
</html> 

</br> </br> </br> </br>
